{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabcaed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import time\n",
    "\n",
    "from configparser import ConfigParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f598911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_file_path = \"/home/datamaking/workarea/code/course_download/ecom-real-time-case-study/realtime_data_processing/\"\n",
    "conf_file_name = conf_file_path + \"datamaking_app.conf\"\n",
    "config_obj = ConfigParser()\n",
    "print(config_obj)\n",
    "print(config_obj.sections())\n",
    "config_read_obj = config_obj.read(conf_file_name)\n",
    "print(type(config_read_obj))\n",
    "print(config_read_obj)\n",
    "print(config_obj.sections())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac020e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka Cluster/Server Details\n",
    "kafka_host_name = config_obj.get('kafka', 'host')\n",
    "kafka_port_no = config_obj.get('kafka', 'port_no')\n",
    "input_kafka_topic_name = config_obj.get('kafka', 'input_topic_name')\n",
    "output_kafka_topic_name = config_obj.get('kafka', 'output_topic_name')\n",
    "kafka_bootstrap_servers = kafka_host_name + ':' + kafka_port_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5b6e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MySQL Database Server Details\n",
    "mysql_host_name = config_obj.get('mysql', 'host')\n",
    "mysql_port_no = config_obj.get('mysql', 'port_no')\n",
    "mysql_user_name = config_obj.get('mysql', 'username')\n",
    "mysql_password = config_obj.get('mysql', 'password')\n",
    "mysql_database_name = config_obj.get('mysql', 'db_name')\n",
    "mysql_driver = config_obj.get('mysql', 'driver')\n",
    "\n",
    "mysql_salesbycardtype_table_name = config_obj.get('mysql', 'mysql_salesbycardtype_tbl')\n",
    "mysql_salesbycountry_table_name = config_obj.get('mysql', 'mysql_salesbycountry_tbl')\n",
    "\n",
    "mysql_jdbc_url = \"jdbc:mysql://\" + mysql_host_name + \":\" + mysql_port_no + \"/\" + mysql_database_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ab770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_properties = {}\n",
    "db_properties['user'] = mysql_user_name\n",
    "db_properties['password'] = mysql_password\n",
    "db_properties['driver'] = mysql_driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876eb571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_mysql_table(current_df, epoc_id, mysql_table_name):\n",
    "    print(\"Inside save_to_mysql_table function\")\n",
    "    print(\"Printing epoc_id: \")\n",
    "    print(epoc_id)\n",
    "    print(\"Printing mysql_table_name: \" + mysql_table_name)\n",
    "\n",
    "    mysql_jdbc_url = \"jdbc:mysql://\" + mysql_host_name + \":\" + str(mysql_port_no) + \"/\" + mysql_database_name\n",
    "\n",
    "    current_df = current_df.withColumn('batch_no', lit(epoc_id))\n",
    "\n",
    "    #Save the dataframe to the table.\n",
    "    current_df.write.jdbc(url = mysql_jdbc_url,\n",
    "                  table = mysql_table_name,\n",
    "                  mode = 'append',\n",
    "                  properties = db_properties)\n",
    "\n",
    "    print(\"Exit out of save_to_mysql_table function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde85546",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Real Time Spark Streaming Kafka Messages\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b98ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a streaming DataFrame that reads from test-topic\n",
    "orders_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", input_kafka_topic_name) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "print(\"Printing Schema of orders_df: \")\n",
    "orders_df.printSchema()\n",
    "\n",
    "orders_df = orders_df.selectExpr(\"CAST(value AS STRING)\", \"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a9a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a schema for the orders data\n",
    "### Sample Data\n",
    "# {'order_id': 1, 'order_product_name': 'Laptop', 'order_card_type': 'MasterCard',\n",
    "# 'order_amount': 38.48, 'order_datetime': '2020-10-21 10:59:10', 'order_country_name': 'Italy',\n",
    "# 'order_city_name': 'Rome', 'order_ecommerce_website_name': 'www.flipkart.com'}\n",
    "\n",
    "orders_schema = StructType() \\\n",
    "    .add(\"order_id\", StringType()) \\\n",
    "    .add(\"order_product_name\", StringType()) \\\n",
    "    .add(\"order_card_type\", StringType()) \\\n",
    "    .add(\"order_amount\", StringType()) \\\n",
    "    .add(\"order_datetime\", StringType()) \\\n",
    "    .add(\"order_country_name\", StringType()) \\\n",
    "    .add(\"order_city_name\", StringType()) \\\n",
    "    .add(\"order_ecommerce_website_name\", StringType())\n",
    "\n",
    "\n",
    "orders_df = orders_df\\\n",
    "    .select(from_json(col(\"value\"), orders_schema)\\\n",
    "    .alias(\"orders\"), \"timestamp\")\n",
    "\n",
    "orders_df.printSchema()\n",
    "\n",
    "# orders -> ['order_id': 1, 'order_product_name': 'Laptop', ....]\n",
    "\n",
    "orders_df = orders_df.select(\"orders.*\", \"timestamp\")\n",
    "\n",
    "orders_df = orders_df.withColumn(\"partition_date\", to_date(\"order_datetime\"))\n",
    "orders_df = orders_df.withColumn(\"partition_hour\", hour(to_timestamp(\"order_datetime\", 'yyyy-MM-dd HH:mm:ss')))\n",
    "\n",
    "print(\"Printing schema of orders_df after creating date & hour column from order_datetime \")\n",
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c7b15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_agg_write_stream_pre = orders_df \\\n",
    "    .writeStream \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"truncate\", \"false\")\\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "orders_agg_write_stream_pre_hdfs = orders_df.writeStream \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"/tmp/data/ecom_data/raw\") \\\n",
    "    .option(\"checkpointLocation\", \"orders-agg-write-stream-pre-checkpoint\") \\\n",
    "    .partitionBy(\"partition_date\", \"partition_hour\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b803e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple aggregate - find total_sales(sum of order_amount) by order_card_type\n",
    "orders_card_df = orders_df.groupBy(\"order_card_type\") \\\n",
    "    .agg({'order_amount': 'sum'}) \\\n",
    "    .select(\"order_card_type\", col(\"sum(order_amount)\") \\\n",
    "    .alias(\"total_sales\"))\n",
    "\n",
    "orders_card_df = orders_card_df.withColumnRenamed(\"order_card_type\",\"card_type\")\n",
    "\n",
    "print(\"Printing Schema of orders_df card-wise: \")\n",
    "orders_card_df.printSchema()\n",
    "\n",
    "orders_card_df \\\n",
    ".writeStream \\\n",
    ".trigger(processingTime='10 seconds') \\\n",
    ".outputMode(\"update\") \\\n",
    ".foreachBatch(lambda current_df, epoc_id: save_to_mysql_table(current_df, epoc_id, mysql_salesbycardtype_table_name)) \\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b34cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple aggregate - find total_sales(sum of order_amount) by order_country_name\n",
    "orders_df_country = orders_card_df.groupBy(\"order_country_name\") \\\n",
    "    .agg({'order_amount': 'sum'}) \\\n",
    "    .select(\"order_country_name\", col(\"sum(order_amount)\") \\\n",
    "    .alias(\"total_sales\"))\n",
    "\n",
    "orders_df_country = orders_df_country.withColumnRenamed(\"order_country_name\",\"country\")\n",
    "\n",
    "print(\"Printing Schema of orders_df grouped by country and card type: \")\n",
    "orders_df_country.printSchema()\n",
    "\n",
    "orders_df_country \\\n",
    ".writeStream \\\n",
    ".trigger(processingTime='10 seconds') \\\n",
    ".outputMode(\"update\") \\\n",
    ".foreachBatch(lambda current_df, epoc_id: save_to_mysql_table(current_df, epoc_id, mysql_salesbycountry_table_name)) \\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f268599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write final result into console for debugging purpose\n",
    "orders_agg_write_stream = orders_card_df \\\n",
    "    .writeStream \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"truncate\", \"false\")\\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "orders_agg_write_stream.awaitTermination()\n",
    "\n",
    "print(\"Real-Time Data Processing Application Completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
